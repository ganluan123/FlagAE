% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/crossvalidation.R
\name{crossvalidation}
\alias{crossvalidation}
\alias{Lossfun}
\alias{kfdpar}
\alias{CVhier}
\alias{CVising}
\title{Loss Calculation by Cross Validation}
\usage{
Lossfun(aedata, PI)

kfdpar(adsl, adae, k)

CVhier(AElist, inits, n_burn, n_iter, thin, n_adapt, n_chain)

CVising(AElist, beta.ab, rho, theta, sim)
}
\arguments{
\item{aedata}{output from function \code{\link{preprocess}}}

\item{PI}{output from function \code{\link{Hiergetpi}} or \code{\link{Isinggetpi}}}

\item{adsl}{subject level analysis dataset, it is a .csv file, it has to contain at least two columns, "USUBJID" and "TRTCTR", "TRTCTR"
is the indicator for treatment and control group. TRTCTR=1 for treatment group and TRTCTR=0 for control group.}

\item{adae}{adverse event analysis dataset, it is a .csv file, it has to contain at least three columns, "USUBJID", "AEBODSYS",and "AEDECOD"}

\item{k}{interger, the number of folds used to split the dataset for cross validation}

\item{inits}{a list, with length equal to n_chain, each element of \code{inits} is
also a list contains initials for each chain; it needs to provide initials for the following
variables: mu.gamma.0, tau.gamma.0, mu.theta.0, tau.theta.0, alpha.pi, beta.pi}

\item{n_burn}{integer, number of interations without saving posterior samples}

\item{n_iter}{integer, number of interations saving posterior samples with every \code{thin}th iteration}

\item{thin}{integer, samples are saved for every \code{thin}th iteration}

\item{n_adapt}{integer, number of adaptations}

\item{n_chain}{number of MCMC chains}

\item{beta.ab}{numeric vector with length of 2, is the prior for beta distribution}

\item{rho}{numeric vector with length equals to the number of rows of data frame aedata}

\item{theta}{numeric, \code{rho} and \code{theta} are parameters for Ising prior}

\item{sim}{numeric vecotr with length of 3, integer for each element, sim[1] is the
number of iterations of brun in; sim[2] is the number of interactions to recorded;
sim[3] is like the parameter thin in MCMC settings. The total number of iterations running is
\eqn{sim[1]+sim[2]*sim[3]}}
}
\value{
\strong{\code{Lossfun}} returns the loss for dataset \code{aedata} based on the fitted incidence \code{PI}.\cr
\strong{\code{kfdpar}} returns a list with k elements with each element is a also a list,
that contains two elements, named traindf and testdf.\cr
\strong{\code{CVhier}} returns the final training and testing loss for Bayesian hierarchical model. \cr
\strong{\code{CVIsing}} returns the final training and testing loss for Bayesian model with Ising model. \cr
}
\description{
Function here are to calculate the loss by cross validation for Bayesian hierarchical model (see also \code{\link{Hier}})
and Bayesian model with Ising prior (see also \code{\link{Ising}}). This can be used to select the best hyperparameters and to compare
two models.
}
\details{
The loss is calcuated by:
\deqn{\sqrt{\sum_{bj} [(Y_{bj}-N_t*t_{bj})^2]}/N_t + \sqrt{\sum_{bj} [(X_{bj}-N_c*c_{bj})^2]}/N_c}
Here b=1,..., B and j=1, ... , k_b, Y_{bj} and X_{bj} are the number of subjects with
an AE with PT j under SOC b in treatment and control groups.
N_t and N_c are the number of subjects in treatment and control groups, respectively.
t_{bj} and c_{bj} are the model fitted incidence of an AE with PT j under SOC b in treatment and control groups.
This formular gives the loss for one interaction/sample, the final loss is the average of loss from all of the interactions/samples.\cr

The loss is calcuated in following way: first the subjects original AE dataset (output of \code{\link{preprocess}}) is randomly evenly
divided k independent subparts. For each subpart, use this subpart as the testing dataset and use the rest of the whole dataset as the
training dataset. Model is trained with the training dataset and then loss is calculated for the testing dataset and training dataset.
Repeat this for each subpart and take the average of the testing loss and training loss from each subpart as the final loss. \cr

\strong{\code{Lossfun}} takes the AE dataset and fitted incidence as parameters and calculate the loss based on the loss function above.\cr

\strong{\code{kfdpar}} first calls function \code{preprocess} to process the data and produce a temporary dataset
and also calls function \code{preprocess} to process the data to get the whole AE dataset.
Then this temporary dataset will be  randomly divided into k equal subparts. For each subpart,
use this subpart as the testing dataset and use the rest of the whole dataset as the
training dataset.This function will generate a list with k elements with each element is a also a list
a list contains two elements, named traindf and testdf.
"traindf" is used to train the model and testdf is usesd to calcualte the loss.
The output is going to be used for further crossvalidation to calculate loss. \cr

\strong{\code{CVhier}} calculates the loss for Bayesian Hierarchical model.\cr

\strong{\code{CVising}} calculates the los for Bayesian model with Ising prior. \cr
}
\examples{
\dontrun{
data(ADAE)
data(ADSL)
AEdata<-preprocess(adsl=ADSL, adae=ADAE)
AELIST<-kfdpar(ADSL, ADAE, k=5)

# Bayesian Hierarchical Model
INITS1<-list(mu.gamma.0=0.1, tau.gamma.0=0.1, mu.theta.0=0.1, tau.theta.0=0.1, alpha.pi=2, beta.pi=2)
INITS2<-list(mu.gamma.0=1, tau.gamma.0=1, mu.theta.0=1, tau.theta.0=1, alpha.pi=10, beta.pi=10)
INITS <- list(INITS1,INITS2)
HIERRAW<-Hier_history(aedata=AEdata, inits=INITS, n_burn=1000, n_iter=1000, thin=20, n_adapt=1000, n_chain=2)
HIERPI<-Hiergetpi(aedata=AEdata, hierraw=HIERRAW)
loss_1<-Lossfun(aedata=AEdata, PI=HIERPI)
LOSSHIER<-CVhier(AElist=AELIST, inits=INITS, n_burn=1000, n_iter=1000, thin=20, n_adapt=1000, n_chain=2)
LOSSHIER$trainloss # train loss
LOSSHIER$testloss # test loss

# Bayesian model with Ising prior
RHO<-rep(1,dim(AEdata)[1])
THETA<-0.02
SIM<-c(5000,1000,20)
BETA.AB<-c(0.25, 0.75)
ISINGRAW<-Ising_history(aedata = AEdata, beta.ab = BETA.AB, rho = RHO, theta = THETA, sim = SIM)
ISINGPI<-Isinggetpi(aedata = AEdata, isingraw=ISINGRAW)
loss_2<-Lossfun(aedata=AEdata, PI=ISINGPI)
LOSSISING<-CVising(AElist=AELIST, beta.ab = BETA.AB, rho = RHO, theta = THETA, sim = SIM)
LOSSISING$trainloss # train loss
LOSSISING$testloss # test loss
}

}
\seealso{
\code{\link{preprocess}}, \code{\link{Hier}}, \code{\link{Ising}}, \code{\link{Isinggetpi}},
\code{\link{Hiergetpi}}
}
